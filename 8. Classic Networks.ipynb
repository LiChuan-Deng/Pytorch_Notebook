{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "bbf726d0b1f933513ea74365d39462104047c8d9bd4636ddbcd10a56e8c32395"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "source": [
    "## LeNet-5\n",
    "> 99.2% acc, 5/6 layers\n",
    "\n",
    "* INPUT (32*32) -\\[convolutions\\]-> C1: feature maps (6@28*28) -\\[subsampling\\]-> S2: feature maps (6@14*14) -\\[convolutions\\]-> C3: feature maps (16@10*10) -\\[subsampling\\]-> S4: feature maps (16@5*5) -\\[full connection\\]-> C5: layer(120) -\\[full connection\\]-> F6: layer (84) -\\[Gaussian connection\\]-> OUTPUT (10)\n",
    "\n",
    "## AlexNet\n",
    "> GTX 580 (3GB*2), 11*11, 8 layers\n",
    "\n",
    "* Similar framework to LeNet but:\n",
    "    * Max pooling, ReLU nonlinearity\n",
    "    * More data and bigger model (7 hidden layers, 650K units, 60M parameters)\n",
    "    * GPU implementation (50x speedup over CPU) - Trained on two GPUs for a week\n",
    "    * Dropout regularization\n",
    "    > A. Krizhevsky, I. Sutskever, and G. Hinton\n",
    "\n",
    "## VGG\n",
    "> 3x3, 1x1, 11-19 layers\n",
    "\n",
    "* Sequence of deeper networks trained progressively\n",
    "* Large receptive fields replaced by successive layers of 3*3 convolutions (with ReLU in between)\n",
    "* One 7*7 convolutions layer with C feature maps needs 49C^2 weights, three 3*3 convolutions layers need only 27C^2 weights\n",
    "* Experimented with 1*1 convolutions\n",
    ">> K. Simonyan and A. Zisserman\n",
    "\n",
    "## GoogLeNet\n",
    "> 1st in 2014 ILSVRC, 22layers\n",
    "\n",
    "* Previous layer -{1*1 convolutions, 3*3 convolutions, 5*5 convolutions, 3*3 max pooling}-> Filter concatenation\n",
    ">> C.Szegedy et al.\n",
    "\n",
    "### Stack more layers?\n",
    "> CIFAR-10 experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## ResNet\n",
    "> 152 layers, ILSVRC 2015\n",
    "\n",
    "* The residual module\n",
    "    * Introduce skip or shortcut connections (existing before in various forms in literature)\n",
    "    * Make it easy for network layers to represent the identity mapping\n",
    "    * For some reason, need to skip at least two layers\n",
    "    \n",
    ">> Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun\n",
    "\n",
    "* Deeper residual module (bottleneck)\n",
    "    * Directly performing 3*3 convolutions with 256 feature maps at input and output: 256*256*3*3 ~ 600K operations\n",
    "    * Using 1*1 convolutions to reduce 256 to 64 feature maps, followed by 3*3 convolutions, followed by 1*1 convolutions to expand back to 256 maps: 256*64*1*1 ~ 16K, 64*64*3*3 ~ 36K, 64*256*1*1 ~ 16K, total: ~70K\n",
    "\n",
    "### DenseNet\n",
    "* More complicated version of ResNet\n",
    "* Introduce skip or shortcut connections between every layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet\n",
    "class ResBlk(nn.Module):\n",
    "    def __int__(self, ch_in, ch_out):\n",
    "        self.conv1 = torch.nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(ch_out)\n",
    "        self.conv2 = torch.nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(ch_out)\n",
    "\n",
    "        self.extra = torch.nn.Sequential()\n",
    "        if ch_out != ch_in:\n",
    "            # [b, ch_in, h, w] => [b, ch_out, h, w]\n",
    "            self.extra = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(ch_in, ch_out, kernel_size=1, stride=1),\n",
    "                torch.nn.BatchNorm2d(ch_out)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.nn.funtional.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.extra(x) + out\n",
    "        return  out\n"
   ]
  },
  {
   "source": [
    "### nn.Module\n",
    "* Magic\n",
    "    * Every layer is nn.Module\n",
    "    > nn.Linear, nn.BatchNorm2d, nn.Conv2d\n",
    "\n",
    "    * nn.Module nested in nn.Module\n",
    "* 1. embed current layers\n",
    "> Linear, ReLU, Sigmoid, Conv2d, ConvTransposed2d, Dropout, etc\n",
    "\n",
    "* 2. container\n",
    "> net(x), nn.Sequential()\n",
    "\n",
    "* 3. parameters\n",
    "> .parameters()\n",
    "\n",
    "* 4. modules\n",
    "    * modules: all nodes\n",
    "    * children: direct children\n",
    "\n",
    "* 5. to(device)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Module\n",
    "class MyLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, inp, outp):\n",
    "        super(MyLinear, self).__init__()\n",
    "\n",
    "        # requires_grad = True\n",
    "        self.w = nn.Parameter(torch.randn(outp, inp))\n",
    "        self.b = nn.Parameter(torch.randn(outp))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x @ self.w.t() + self.b\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# container\n",
    "self.net = nn.Sequential(\n",
    "    nn.Conv2d(1,32,5,1,1),\n",
    "    nn.MaxPool2d(2,2),\n",
    "    nn.ReLU(True),\n",
    "    nn.BatchNorm2d(32),\n",
    "\n",
    "    nn.Conv2d(32,64,3,1,1),\n",
    "    nn.ReLU(True),\n",
    "    nn.BatchNorm2d(64),\n",
    "\n",
    "    nn.Conv2d(64,64,3,1,1),\n",
    "    nn.MaxPool2d(2,2),\n",
    "    nn.ReLU(True),\n",
    "    nn.BatchNorm2d(64),\n",
    "\n",
    "    nn.Conv2d(64,128,3,1,1),\n",
    "    nn.ReLU(True),\n",
    "    nn.BatchNorm2d(128),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# parameters\n",
    "net = torch.nn.Sequential(torch.nn.Linear(4,2),torch.nn.Linear(2,2))\n",
    "list(net.parameters())[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('0.weight',\n",
       " Parameter containing:\n",
       " tensor([[-0.0526,  0.1836, -0.2847,  0.4368],\n",
       "         [ 0.4340, -0.0786,  0.3293,  0.2472]], requires_grad=True))"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "list(net.named_parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_items([('0.weight', Parameter containing:\n",
       "tensor([[-0.0526,  0.1836, -0.2847,  0.4368],\n",
       "        [ 0.4340, -0.0786,  0.3293,  0.2472]], requires_grad=True)), ('0.bias', Parameter containing:\n",
       "tensor([-0.3175,  0.2888], requires_grad=True)), ('1.weight', Parameter containing:\n",
       "tensor([[ 0.1183, -0.6117],\n",
       "        [-0.2555,  0.4316]], requires_grad=True)), ('1.bias', Parameter containing:\n",
       "tensor([0.0416, 0.2182], requires_grad=True))])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "dict(net.named_parameters()).items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules\n",
    "class BasicNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicNet, self).__init__()\n",
    "        self.net = nn.Linear(4,3)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(BasicNet(),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(3,2))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Net(\n  (net): Sequential(\n    (0): BasicNet(\n      (net): Linear(in_features=4, out_features=3, bias=True)\n    )\n    (1): ReLU()\n    (2): Linear(in_features=3, out_features=2, bias=True)\n  )\n), Sequential(\n  (0): BasicNet(\n    (net): Linear(in_features=4, out_features=3, bias=True)\n  )\n  (1): ReLU()\n  (2): Linear(in_features=3, out_features=2, bias=True)\n), BasicNet(\n  (net): Linear(in_features=4, out_features=3, bias=True)\n), Linear(in_features=4, out_features=3, bias=True), ReLU(), Linear(in_features=3, out_features=2, bias=True)]\n--------------------------------------------\n[Sequential(\n  (0): BasicNet(\n    (net): Linear(in_features=4, out_features=3, bias=True)\n  )\n  (1): ReLU()\n  (2): Linear(in_features=3, out_features=2, bias=True)\n)]\n"
     ]
    }
   ],
   "source": [
    "m = Net()\n",
    "print(list(m.modules()))\n",
    "print('--------------------------------------------')\n",
    "print(list(m.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}