{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Overfitting\n",
    "* Occam's Razor\n",
    "    * More things should not be used than are necessary\n",
    "* Reduce Overfitting\n",
    "    * more data\n",
    "    * constraint model complexity\n",
    "        * shallow\n",
    "        * regularization\n",
    "    * dropout\n",
    "    * data argumentation\n",
    "    * early stopping"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Regularization\n",
    "##### J(zeta) = CrossEntropyLoos + lambda * sum(|zeta_i|)\n",
    "* where lambda = 0.01, zeta is the parameters of model\n",
    "* enforce weights close to 0 -> weight decay"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### L1-regularization\n",
    "* J(zeta) = CrossEntropyLoos + lambda * sum(|zeta_i|)\n",
    "#### L2-regularization\n",
    "* J(W;X,y) + 1/2 * lambda * ||W||^2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 regularization\n",
    "regularization_loss = 0\n",
    "for param in model.parameters():\n",
    "    regularization_loss += torch.sum(torch.abs(param))\n",
    "\n",
    "classify_loss = criteon(logits, target)\n",
    "loss = classify_loss + 0.01 * regularization_loss\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 regularization\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, weight_decay=0.01)"
   ]
  },
  {
   "source": [
    "### Tricks\n",
    "* momentum\n",
    "    * before: w_k+1 = w_k - alph * grad_f(w_k)\n",
    "    * z_k+1 = beta * z_k + grad_f(w_k),    w_k+1 = w_k - alph * z_k+1\n",
    "* learning rate decay    \n",
    "    * A small learning rate requires many updates before reaching the minimum point\n",
    "    * The optimal learning rate swiftly reaches the minimum point\n",
    "    * Too large of a learning rate causes drastic updates which lead to divergent behaviors\n",
    "\n",
    "> 3e-4 is the best learning rate for Adam"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# momentum\n",
    "optimizer = torch.optim.SGD(model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer,'min')\n",
    "\n",
    "for epoch in xrange(args.start_epoch, args.epochs):\n",
    "    train(train_loader, model, criterion, optimizer, epoch)\n",
    "    result_avg, loss_val = validate(val_loader, model, criterion, epoch)\n",
    "    scheduler.step(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming optimizer uses lr = 0.05 for all groups\n",
    "# lr = 0.05     if epoch < 30\n",
    "# lr = 0.005    if 30 <= epoch < 60\n",
    "# ...\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "for epoch in range (100):\n",
    "    scheduler.step()\n",
    "    train(...)\n",
    "    validate(...)"
   ]
  },
  {
   "source": [
    "### Tricks\n",
    "* Early Stopping\n",
    "    * Validation set to select parameters\n",
    "    * Monitor validation performance\n",
    "    * Stop at the highest val perf. **(Experience)**\n",
    "* Dropout\n",
    "    * Learning less to learn better\n",
    "    * Each connection has p = \\[0,1\\] to lose\n",
    "* Stochastic Gradient Descent\n",
    "    * Stochastic is not random!\n",
    "    * Deterministic\n",
    "    * Because of the limit of GPU memory, gradient descent in range of batch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout\n",
    "net_dropped = torch.nn.Sequential(\n",
    "    torch.nn.Linear(784,200),\n",
    "    torch.nn.Dropout(0.5),  # drop 50% of the neuron\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(200,200),\n",
    "    torch.nn.Dropout(0.5),  # drop 50% of the neuron\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(200,10)\n",
    ")\n",
    "\n",
    "## torch.nn.Dropout(p=dropout_prob)\n",
    "## tf.nn.dropout(keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Behavior between train and test\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # train\n",
    "    net_dropped.train()\n",
    "    for batch_indx, (data, traget) in enumerate(train_loader):\n",
    "        ...\n",
    "    net_dropped.eval() # before test, swich to connection\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, traget in test_loader:\n",
    "        ...\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}