{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "bbf726d0b1f933513ea74365d39462104047c8d9bd4636ddbcd10a56e8c32395"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "source": [
    "## Activation Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### sigmoid\n",
    "sigmoid(x) = 1/(1+e^(-x))    in (0,1)\n",
    "\n",
    "d sigmoid/d x = sigmoid(x)(1-sigmoid(x))"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 1.6655e-34, 7.4564e-25, 3.3382e-15, 1.4945e-05, 9.9999e-01,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "a = torch.linspace(-100,100,10)\n",
    "torch.sigmoid(a)"
   ]
  },
  {
   "source": [
    "### Tanh\n",
    "tanh(x) = (e^x-e^(-x))/(e^x+e^(-x)) = 2sigmoid(2x)-1    in (-1,1)\n",
    "\n",
    "d tanh(x)/d x = 1-tanh^2(x)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([-0.7616, -0.6514, -0.5047, -0.3215, -0.1107,  0.1107,  0.3215,  0.5047,\n",
       "         0.6514,  0.7616])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "a = torch.linspace(-1,1,10)\n",
    "torch.tanh(a)"
   ]
  },
  {
   "source": [
    "### Rectified Linear Unit(ReLU)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1111, 0.3333, 0.5556, 0.7778,\n",
       "        1.0000])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "torch.relu(a)"
   ]
  },
  {
   "source": [
    "## Loss Functions\n",
    "* Mean Squared Error\n",
    "* Cross Entropy Loss\n",
    "    * binary\n",
    "    * multi-class\n",
    "    * +softmax\n",
    "    * Leave it to Logistic Regression Part\n",
    "* torch.autograd.grad(loss,\\[w1,w2,...\\])\n",
    "    * \\[w1 grad, w2 grad,...\\]\n",
    "* loss.backward()\n",
    "    * w1.grad\n",
    "    * w2.grad"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "x = torch.ones(1)\n",
    "w = torch.full([1],2.)\n",
    "mse = F.mse_loss(torch.ones(1),x*w)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-8ffc13237761>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\JetBrains\\environment\\Anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m     return Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    224\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         inputs, allow_unused, accumulate_grad=False)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "torch.autograd.grad(mse,[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([2.]),)"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "w.requires_grad_()\n",
    "mse = F.mse_loss(torch.ones(1),x*w)\n",
    "torch.autograd.grad(mse,[w])"
   ]
  },
  {
   "source": [
    "### loss.backward"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([2.])"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "mse = F.mse_loss(torch.ones(1),x*w)\n",
    "mse.backward()\n",
    "w.grad"
   ]
  },
  {
   "source": [
    "### Softmax\n",
    "* soft version of max\n",
    "\n",
    "    S(y_i) = e^(y_i)/sum_j(e^(y_j))\n",
    "\n",
    "    d S(y_i)/d y_j = S(y_j)(1-S(y_j)), i = j\n",
    "\n",
    "    d S(y_i)/d y_j = -S(y_j)S(y_i)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.9387, 0.6220, 0.5901], requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "a = torch.rand(3)\n",
    "a.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((tensor([-0.1230,  0.2097, -0.0868]),),\n",
       " (tensor([-0.1191, -0.0868,  0.2059]),))"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "p = F.softmax(a,dim=0)\n",
    "torch.autograd.grad(p[1],[a],retain_graph=True), torch.autograd.grad(p[2],[a])\n",
    "# loss should be [1] scale"
   ]
  },
  {
   "source": [
    "### Perceptron"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "x = torch.randn(1,10)\n",
    "w = torch.randn(1,10,requires_grad=True)\n",
    "o = torch.sigmoid(x@w.t())\n",
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "loss = F.mse_loss(torch.ones(1,1),o)\n",
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.0905,  0.3336,  0.0114, -0.5023,  0.0928,  0.0549, -0.0823, -0.0502,\n",
       "         -0.2528, -0.1147]])"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "loss.backward()\n",
    "w.grad"
   ]
  },
  {
   "source": [
    "### MLP and Grad"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "x = torch.randn(1,10)\n",
    "w = torch.randn(2,10,requires_grad=True)\n",
    "o = torch.sigmoid(x@w.t())\n",
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.7157, grad_fn=<MseLossBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "loss = F.mse_loss(torch.ones(1,2),o)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.0116, -0.1854, -0.1572,  0.0973, -0.1311, -0.1948, -0.1331, -0.0420,\n",
       "          0.0169, -0.0711],\n",
       "        [-0.0014, -0.0224, -0.0190,  0.0118, -0.0158, -0.0235, -0.0161, -0.0051,\n",
       "          0.0020, -0.0086]])"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "loss.backward()\n",
    "w.grad"
   ]
  },
  {
   "source": [
    "### Chain rule\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "# check\n",
    "x = torch.tensor(1.)\n",
    "w1 = torch.tensor(2.,requires_grad=True)\n",
    "b1 = torch.tensor(1.)\n",
    "w2 = torch.tensor(2.,requires_grad=True)\n",
    "b2 = torch.tensor(1.)\n",
    "\n",
    "y1 = x*w1+b1\n",
    "y2 = y1*w2+b2\n",
    "\n",
    "dy2_dy1 = torch.autograd.grad(y2,[y1],retain_graph=True)[0]\n",
    "dy1_dw1 = torch.autograd.grad(y1,[w1],retain_graph=True)[0]\n",
    "dy2_dw1 = torch.autograd.grad(y2,[w1],retain_graph=True)[0]\n",
    "\n",
    "dy2_dy1*dy1_dw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "dy2_dw1"
   ]
  },
  {
   "source": [
    "## Logistic Regression\n",
    "* Recap\n",
    "    * for continuous: y = xw + b\n",
    "    * for probability output: y = sigmoid(xw + b)\n",
    "* Binary Classification\n",
    "    * interpret network as f: x -> p(y|x;zita)\n",
    "    * output in \\[0,1\\]\n",
    "    * which is exactly what *logistic function* cones in!\n",
    "\n",
    "* Goal v.s. Approach\n",
    "    * For regression:\n",
    "        * Goal: *pred* = y\n",
    "        * Approach: minimize *dist(pred,y)*\n",
    "    * For classification:\n",
    "        * Goal: maximize benchmark, e.g. *accuracy*\n",
    "        * Approach 1: minimize *dist(p_zita(y|x),p_r(y|x))*\n",
    "        * Approach 2: minimize *divergence(p_zita(y|x),p_r(y|x))*\n",
    "\n",
    "* Q1. why not maximize accuracy?\n",
    "    * *acc.* = sum(I(pred_i ==y_i))/len(Y)\n",
    "    * issues 1. gradient = 0 if accuracy unchanged but weights changed\n",
    "    * issues 2. gradient not continuous since the number of correct is not continuous\n",
    "\n",
    "* Q2. why call logistic regression\n",
    "    * use sigmoid\n",
    "    * controversial!\n",
    "        * MSE => regression\n",
    "        * Cross Entropy => classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Loss for classification\n",
    "* MSE\n",
    "* Cross Entropy Loss\n",
    "* Hinge Loss\n",
    "    sum(max(0,1-y_i*h_zita(x_i)))"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Entropy\n",
    "* Uncertainty\n",
    "* measure of surprise\n",
    "* higher entropy = less info.\n",
    "    * *Entropy* = - sum_i(P(i)logP(i))\n",
    "\n",
    "### Cross Entropy\n",
    "* H(p,q) = - sum_i(P(i)logq(i))\n",
    "* H(p,q) = H(p) + D_KL(p|q)\n",
    "* P = Q\n",
    "    * Cross Entropy = Entropy <= KL divergence = 0\n",
    "* for onr-hot encoding\n",
    "    * entropy = 1log1 = 0\n",
    "* why not use MSE\n",
    "    * sigmoid + MSE => gradient vanish\n",
    "    * converge slower\n",
    "    * but sometimes: e.g. meta-learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1,784)\n",
    "w = torch.randn(10,784)\n",
    "\n",
    "logits = x@w.t()\n",
    "pred = F.softmax(logits, dim=1)\n",
    "\n",
    "pre_log = torch.log(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor(87.2855), tensor(87.2855))"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "F.cross_entropy(logits,torch.tensor([3])), F.nll_loss(pre_log,torch.tensor([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}