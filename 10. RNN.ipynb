{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0bbf726d0b1f933513ea74365d39462104047c8d9bd4636ddbcd10a56e8c32395",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "source": [
    "### Sequence representation\n",
    "> \\[seq_len, feature_len\\], \\[words, word_vec\\]\n",
    "\n",
    "* one-hot to represent a word\n",
    "> sparse and high-dim\n",
    "\n",
    "* semantic similarity\n",
    "> word2vec, GloVe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.5031,  0.7606,  3.1124,  0.6215, -0.1789]],\n       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "\n",
    "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "\n",
    "embeds = torch.nn.Embedding(2,5) # 2 words in vocab, 5 dimensional embeddings\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "source": [
    "# GloVe\n",
    "from torchnlp.word_to_vector import GloVe\n",
    "vectors = GloVe()\n",
    "vectors['hello']"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### Consistent memory\n",
    "> h_t = f_w(h_t-1, x_t), h_t = tanh(W_hh * h_t-1 + W_xh * x_t), y_t = W_hy * h_t"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Folded model\n",
    "* h_t+1 = x_t @ W_xh + h_t @ W_hh\n",
    "> \\[batch, feature len\\] @ \\[hidden len, feature len\\] + \\[batch, hidden len\\] @ \\[hidden len, hidden len\\]\n",
    ">> x_t: \\[batch, feature len\\], x: \\[seq len, batch, feature len\\]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "odict_keys(['weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0'])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# input dim, hidden dim\n",
    "rnn = torch.nn.RNN(100,10) # memory dim, word dim\n",
    "rnn._parameters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([10, 10]), torch.Size([10, 100]))"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "rnn.weight_hh_l0.shape, rnn.weight_ih_l0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([10]), torch.Size([10]))"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "rnn.bias_hh_l0.shape, rnn.bias_ih_l0.shape"
   ]
  },
  {
   "source": [
    "### nn.RNN\n",
    "* \\__init\\__\n",
    "    * *input_size*: The number of expected features in the input x \n",
    "    * *hidden_size*: The number of features in the hidden state h\n",
    "    * *num_layers*: The number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results.Default: 1\n",
    "* out, ht = forward(x, h0)\n",
    "    * x: \\[seq len, b, word vec\\]\n",
    "    * h0/ht: \\[num layers, b, h dim\\]\n",
    "    * out: \\[seq len, b, h dim\\]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RNN(100, 20)\ntorch.Size([10, 3, 20]) torch.Size([1, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "# Single layer RNN\n",
    "rnn = torch.nn.RNN(input_size=100, hidden_size=20, num_layers=1)\n",
    "print(rnn)\n",
    "\n",
    "x = torch.randn(10, 3, 100)\n",
    "out, h = rnn(x, torch.zeros(1, 3, 20))\n",
    "print(out.shape, h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}