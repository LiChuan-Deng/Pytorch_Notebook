{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0bbf726d0b1f933513ea74365d39462104047c8d9bd4636ddbcd10a56e8c32395",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "### The problem of long-term dependencies\n",
    "* RNNs connect previous information to present task:\n",
    "> enough for predicting the next word for \"the clouds are in the **sky**\"\n",
    "\n",
    "> may not be enough when more context is needed: \"I grew up in France... I speak fluent **French**\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### RNN\n",
    "* All recurrent neural networks have the form of a chain of repeating modules of neural network\n",
    "> ht = tanh(W\\[ht-1, xt-1\\])\n",
    "\n",
    "### LSTM\n",
    "* LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer there are four, interacting in a very special way.\n",
    "* The core idea behind LSTMs: **Cell State**\n",
    "> Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation. An LSTM has three of these gates, to protect and control the cell state\n",
    "\n",
    "#### LSTM: Forget gate\n",
    "> ft = sigmoid(Wf\\[ht-1, xt\\] + bf)\n",
    ">> It looks at ht-1 and xt and outputs a number between 0 and 1 for each number in the cell state Ct-1.\n",
    ">> A 1 represents \"completely keep this\" while a 0 represents \"completely get rid of this\"\n",
    "\n",
    "#### LSTM: Input gate and Cell state\n",
    "* The next step is to decide what new information we're going to store in the cell state\n",
    "> a sigmoid layer called the \"**input gate layer** decides which values we'll update.\"\n",
    ">> it = sigmoid(Wi\\[ht-1, xt\\] + bi)\n",
    "\n",
    "> a tanh layer creates a vector of new candidate values, that could be added to the state.\n",
    ">> Ct^ = tanh(WC\\[ht-1, xt\\] + bC)\n",
    "\n",
    "* It's now time to update the old cell state into the new cell state\n",
    "> Ct = ft * Ct-1 + it * Ct^\n",
    ">> We multiply the old state by ft forgatting the things we decided to forget earlier. Then, we add the new candidate values, scaled by how much we decided to update each state value.\n",
    "\n",
    "#### LSTM: Output\n",
    "* Finally, we need to decide what we're going to output.\n",
    "> First, we run a sigmoid layer which decides what parts of the cell state we're going to output.\n",
    ">> ot = sigmoid(Wo\\[ht-1, xt\\] + bo)\n",
    "\n",
    "> Then, we put the cell state through tanh (to push the values to be between -1 and 1) and multiply it by the output of sigmoid gate, so that we only output the parts we decided to.\n",
    ">> ht = ot * tanh(Ct)\n",
    "\n",
    "#### Intuitive Pipeline\n",
    "* LSTM memory Cell\n",
    "> Forget irrelevant parts of previous state --> Selectively update cell state values --> Output certain parts of cell state\n",
    "\n",
    "* input gate : forget gate : behavior \n",
    "> o : 1 : remember the previous value, 1 : 1 : add to the previous value, 0 : 0 : erase the value, 1 : 0 : overwrite the value\n",
    "\n",
    "### How to solve Gradient Vanishing?\n",
    "d Ct/ d Ct-1 becomes the sum of four elements, while d hk/d h1 is a multiplication that can be exploding or vanishing if many enough elements < 1 or > 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### nn.LSTM\n",
    "* __init__\n",
    "    * *input_size*: The number of expected features in the input x\n",
    "    * *hidden_size*: The number of features in the hidden state h\n",
    "    * *num_layers*: Number of recurrent layers. E.g., setting *num_layers=2* would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1\n",
    "\n",
    "### LSTM.forward()\n",
    "* out, (ht, ct) = lstm(x, \\[ht_0, ct_0\\])\n",
    "    * x: \\[seq, b, vec\\]\n",
    "    * h/c: \\[num_layer, b, h\\]\n",
    "    * out: \\[seq, b, h\\]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### nn.LSTMCell\n",
    "* __init__\n",
    "    * *input_size*: The number of expected features in the input x\n",
    "    * *hidden_size*: The number of features in the hidden state h\n",
    "    * *num_layers*: Number of recurrent layers. E.g., setting *num_layers=2* would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1\n",
    "\n",
    "### LSTMCell.forward()\n",
    "* ht, ct = lstmcell(xt,\\[ht_0, ct_0\\])\n",
    "    * xt: \\[b, vec\\]\n",
    "    * ht/ct: \\[b, h\\]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LSTM(100, 20, num_layers=4)\ntorch.Size([10, 3, 20]) torch.Size([4, 3, 20]) torch.Size([4, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(input_size=100, hidden_size=20, num_layers=4)\n",
    "print(lstm)\n",
    "x = torch.randn(10, 3, 100)\n",
    "out, (h, c) = lstm(x)\n",
    "print(out.shape, h.shape, c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 20]) torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "# single layer\n",
    "cell = nn.LSTMCell(input_size=100, hidden_size=20)\n",
    "h = torch.zeros(3, 20)\n",
    "c = torch.zeros(3,20)\n",
    "for xt in x:\n",
    "    h, c = cell(xt, [h,c])\n",
    "print(h.shape, c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 20]) torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "# two layers\n",
    "cell1 = nn.LSTMCell(input_size=100, hidden_size=30)\n",
    "cell2 = nn.LSTMCell(input_size=30, hidden_size=20)\n",
    "h1 = torch.zeros(3,30)\n",
    "c1 = torch.zeros(3,30)\n",
    "h2 = torch.zeros(3,20)\n",
    "c2 = torch.zeros(3,20)\n",
    "for xt in x:\n",
    "    h1, c1 = cell1(xt, [h1,c1])\n",
    "    h2, c2 = cell2(h1, [h2,c2])\n",
    "print(h2.shape, c2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in google colab\n",
    "!pip install torch\n",
    "!pip install torchtext\n",
    "!python -m spacy download en\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchtext import data, datasets\n",
    "# load dataset\n",
    "TEXT = data.Field(tokenize='spacy')\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL) # torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embedding\n",
    "from rnn import RNN\n",
    "rnn = RNN(len(TEXT.vocab), 100, 256)\n",
    "\n",
    "pretrained_embedding = TEXT.vocab.vectors\n",
    "print('pretrained_embedding:', pretrained_embedding.shape)\n",
    "rnn.embedding.weight.data.copy_(pretrained_embedding)\n",
    "print('embedding layer inited.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "def train(rnn, iterator, optimizer, criteon):\n",
    "    avg_acc = []\n",
    "    rnn.train()\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        # [seq, b] => [b, 1] => [b]\n",
    "        pred = rnn(batch.text).squeeze(1)\n",
    "        loss = criteon(pred, batch.label)\n",
    "        acc = binary_acc(pred, batch.label).item()\n",
    "        avg_acc.append(acc)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# test\n",
    "def binary_acc(pred, y):\n",
    "    preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = torch.eq(preds, y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def eval(rnn, iterator, criteon):\n",
    "    avg_acc = []\n",
    "    rnn.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            # [b, 1] => [b]\n",
    "            pred = rnn(batch.text).squeeze(1)\n",
    "            loss = criteon(pred, batch.label)\n",
    "            acc = binary_acc(pred, batch.label).item()\n",
    "            avg_acc.append(acc)\n",
    "    avg_acc = np.array(avg_acc).mean()\n",
    "    print('>>test:', avg_acc)"
   ]
  }
 ]
}