{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### The problem of long-term dependencies\n",
    "* RNNs connect previous information to present task:\n",
    "> enough for predicting the next word for \"the clouds are in the **sky**\"\n",
    "\n",
    "> may not be enough when more context is needed: \"I grew up in France... I speak fluent **French**\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### RNN\n",
    "* All recurrent neural networks have the form of a chain of repeating modules of neural network\n",
    "> ht = tanh(W\\[ht-1, xt-1\\])\n",
    "\n",
    "### LSTM\n",
    "* LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer there are four, interacting in a very special way.\n",
    "* The core idea behind LSTMs: **Cell State**\n",
    "> Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation. An LSTM has three of these gates, to protect and control the cell state\n",
    "\n",
    "#### LSTM: Forget gate\n",
    "> ft = sigmoid(Wf\\[ht-1, xt\\] + bf)\n",
    ">> It looks at ht-1 and xt and outputs a number between 0 and 1 for each number in the cell state Ct-1.\n",
    ">> A 1 represents \"completely keep this\" while a 0 represents \"completely get rid of this\"\n",
    "\n",
    "#### LSTM: Input gate and Cell state\n",
    "* The next step is to decide what new information we're going to store in the cell state\n",
    "> a sigmoid layer called the \"**input gate layer** decides which values we'll update.\"\n",
    ">> it = sigmoid(Wi\\[ht-1, xt\\] + bi)\n",
    "\n",
    "> a tanh layer creates a vector of new candidate values, that could be added to the state.\n",
    ">> Ct^ = tanh(WC\\[ht-1, xt\\] + bC)\n",
    "\n",
    "* It's now time to update the old cell state into the new cell state\n",
    "> Ct = ft * Ct-1 + it * Ct^\n",
    ">> We multiply the old state by ft forgatting the things we decided to forget earlier. Then, we add the new candidate values, scaled by how much we decided to update each state value.\n",
    "\n",
    "#### LSTM: Output\n",
    "* Finally, we need to decide what we're going to output.\n",
    "> First, we run a sigmoid layer which decides what parts of the cell state we're going to output.\n",
    ">> ot = sigmoid(Wo\\[ht-1, xt\\] + bo)\n",
    "\n",
    "> Then, we put the cell state through tanh (to push the values to be between -1 and 1) and multiply it by the output of sigmoid gate, so that we only output the parts we decided to.\n",
    ">> ht = ot * tanh(Ct)\n",
    "\n",
    "#### Intuitive Pipeline\n",
    "* LSTM memory Cell\n",
    "> Forget irrelevant parts of previous state --> Selectively update cell state values --> Output certain parts of cell state\n",
    "\n",
    "* input gate : forget gate : behavior \n",
    "> o : 1 : remember the previous value, 1 : 1 : add to the previous value, 0 : 0 : erase the value, 1 : 0 : overwrite the value"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}